{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 91.77753406309057,
  "model_size": 49.50337890625,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9815288434214265, \"recall\": 0.9314994606256742, \"f1-score\": 0.9558599695585998, \"support\": 3708}, \"text\": {\"precision\": 0.8661043753294676, \"recall\": 0.961943793911007, \"f1-score\": 0.9115117891816921, \"support\": 1708}, \"accuracy\": 0.9411004431314623, \"macro avg\": {\"precision\": 0.9238166093754471, \"recall\": 0.9467216272683406, \"f1-score\": 0.9336858793701459, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9451283649315695, \"recall\": 0.9411004431314623, \"f1-score\": 0.9418742435460891, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9336858793701459,
  "roc-auc_cpp_researcher_1": 0.9467216272683406,
  "perf_predict_runtime_cpp_researcher_1": 0.47840373998042196,
  "timeit_runtime_cpp_researcher_1": 0.5037551189074293,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9787354692373121, \"recall\": 0.9282065071255714, \"f1-score\": 0.9528015456803753, \"support\": 3719}, \"text\": {\"precision\": 0.8588048651507139, \"recall\": 0.9558563861094762, \"f1-score\": 0.9047353760445683, \"support\": 1699}, \"accuracy\": 0.9368770764119602, \"macro avg\": {\"precision\": 0.9187701671940129, \"recall\": 0.9420314466175238, \"f1-score\": 0.9287684608624718, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9411271088934343, \"recall\": 0.9368770764119602, \"f1-score\": 0.937728747191775, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9287684608624718,
  "roc-auc_cpp_researcher_2": 0.9420314466175238,
  "perf_predict_runtime_cpp_researcher_2": 0.4860974570037797,
  "timeit_runtime_cpp_researcher_2": 0.5162453500903211,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.986322869955157, \"recall\": 0.9383532423208191, \"f1-score\": 0.9617402710975077, \"support\": 4688}, \"text\": {\"precision\": 0.8633569739952719, \"recall\": 0.9676735559088501, \"f1-score\": 0.912543728135932, \"support\": 1887}, \"accuracy\": 0.9467680608365019, \"macro avg\": {\"precision\": 0.9248399219752144, \"recall\": 0.9530133991148346, \"f1-score\": 0.9371419996167198, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9510321253656052, \"recall\": 0.9467680608365019, \"f1-score\": 0.9476210503266342, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9371419996167198,
  "roc-auc_java_researcher_1": 0.9530133991148345,
  "perf_predict_runtime_java_researcher_1": 0.5232978910207748,
  "timeit_runtime_java_researcher_1": 0.49707284769974647,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9838673537978938, \"recall\": 0.9398544520547946, \"f1-score\": 0.9613574165298303, \"support\": 4672}, \"text\": {\"precision\": 0.8671394799054374, \"recall\": 0.9622245540398741, \"f1-score\": 0.9122108928127332, \"support\": 1906}, \"accuracy\": 0.9463362724232289, \"macro avg\": {\"precision\": 0.9255034168516656, \"recall\": 0.9510395030473343, \"f1-score\": 0.9367841546712817, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9500450175803472, \"recall\": 0.9463362724232289, \"f1-score\": 0.9471170282347882, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9367841546712817,
  "roc-auc_java_researcher_2": 0.9510395030473343,
  "perf_predict_runtime_java_researcher_2": 0.4674497719388455,
  "timeit_runtime_java_researcher_2": 0.46957048660842704,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9750160153747598, \"recall\": 0.9395061728395062, \"f1-score\": 0.9569317824583465, \"support\": 3240}, \"text\": {\"precision\": 0.904296875, \"recall\": 0.9595854922279793, \"f1-score\": 0.931121166415284, \"support\": 1930}, \"accuracy\": 0.9470019342359768, \"macro avg\": {\"precision\": 0.9396564451873799, \"recall\": 0.9495458325337427, \"f1-score\": 0.9440264744368152, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9486160268015903, \"recall\": 0.9470019342359768, \"f1-score\": 0.9472964847865649, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9440264744368152,
  "roc-auc_javascript_researcher_1": 0.9495458325337428,
  "perf_predict_runtime_javascript_researcher_1": 0.31218722800258547,
  "timeit_runtime_javascript_researcher_1": 0.31441737259738145,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9746632456703015, \"recall\": 0.9423255813953488, \"f1-score\": 0.9582216616742866, \"support\": 3225}, \"text\": {\"precision\": 0.9092240117130308, \"recall\": 0.9593202883625128, \"f1-score\": 0.9336006013530443, \"support\": 1942}, \"accuracy\": 0.9487129862589511, \"macro avg\": {\"precision\": 0.9419436286916661, \"recall\": 0.9508229348789308, \"f1-score\": 0.9459111315136655, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.950068124256518, \"recall\": 0.9487129862589511, \"f1-score\": 0.9489679169202994, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9459111315136655,
  "roc-auc_javascript_researcher_2": 0.9508229348789308,
  "perf_predict_runtime_javascript_researcher_2": 0.32143635698594153,
  "timeit_runtime_javascript_researcher_2": 0.32356395170791075,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.987747653806048, \"recall\": 0.8965925224798864, \"f1-score\": 0.9399652691639792, \"support\": 4226}, \"text\": {\"precision\": 0.8143585386576041, \"recall\": 0.9760692464358453, \"f1-score\": 0.887911069939787, \"support\": 1964}, \"accuracy\": 0.9218093699515347, \"macro avg\": {\"precision\": 0.9010530962318261, \"recall\": 0.9363308844578658, \"f1-score\": 0.9139381695518831, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.932733724540855, \"recall\": 0.9218093699515347, \"f1-score\": 0.9234492033681289, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.9139381695518831,
  "roc-auc_php_researcher_1": 0.9363308844578659,
  "perf_predict_runtime_php_researcher_1": 0.3829063050216064,
  "timeit_runtime_php_researcher_1": 0.4008702335995622,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9692948217538382, \"recall\": 0.8873272987136732, \"f1-score\": 0.9265016788956598, \"support\": 4198}, \"text\": {\"precision\": 0.7983802216538789, \"recall\": 0.940733299849322, \"f1-score\": 0.8637306894166474, \"support\": 1991}, \"accuracy\": 0.9045079980610761, \"macro avg\": {\"precision\": 0.8838375217038585, \"recall\": 0.9140302992814976, \"f1-score\": 0.8951161841561537, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.9143116308023083, \"recall\": 0.9045079980610761, \"f1-score\": 0.9063082647653136, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8951161841561537,
  "roc-auc_php_researcher_2": 0.9140302992814977,
  "perf_predict_runtime_php_researcher_2": 0.38283230795059353,
  "timeit_runtime_php_researcher_2": 0.3800122829969041,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9874239350912779, \"recall\": 0.8753296571565572, \"f1-score\": 0.9280040668488276, \"support\": 8342}, \"text\": {\"precision\": 0.7033656588705077, \"recall\": 0.9636576787807737, \"f1-score\": 0.8131904369332233, \"support\": 2559}, \"accuracy\": 0.8960645812310797, \"macro avg\": {\"precision\": 0.8453947969808928, \"recall\": 0.9194936679686654, \"f1-score\": 0.8705972518910254, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9207415088139685, \"recall\": 0.8960645812310797, \"f1-score\": 0.9010516699169835, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8705972518910254,
  "roc-auc_python_researcher_1": 0.9194936679686655,
  "perf_predict_runtime_python_researcher_1": 0.6814612050075084,
  "timeit_runtime_python_researcher_1": 0.6941014027921483,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9846174424826111, \"recall\": 0.873501839325976, \"f1-score\": 0.9257372822737848, \"support\": 8427}, \"text\": {\"precision\": 0.696036498431708, \"recall\": 0.9550078247261345, \"f1-score\": 0.8052119412831931, \"support\": 2556}, \"accuracy\": 0.8924701811891105, \"macro avg\": {\"precision\": 0.8403269704571595, \"recall\": 0.9142548320260553, \"f1-score\": 0.865474611778489, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.917457932968443, \"recall\": 0.8924701811891105, \"f1-score\": 0.8976882272276268, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.865474611778489,
  "roc-auc_python_researcher_2": 0.9142548320260554,
  "perf_predict_runtime_python_researcher_2": 0.6791175489779562,
  "timeit_runtime_python_researcher_2": 0.732460207294207
}