{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 252.63264513702597,
  "model_size": 34.5516005859375,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9835379464285714, \"recall\": 0.9506472491909385, \"f1-score\": 0.9668129456939112, \"support\": 3708}, \"text\": {\"precision\": 0.9001091703056768, \"recall\": 0.9654566744730679, \"f1-score\": 0.931638418079096, \"support\": 1708}, \"accuracy\": 0.955317577548006, \"macro avg\": {\"precision\": 0.9418235583671242, \"recall\": 0.9580519618320031, \"f1-score\": 0.9492256818865036, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.957227689852149, \"recall\": 0.955317577548006, \"f1-score\": 0.9557202401610263, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9492256818865036,
  "roc-auc_cpp_researcher_1": 0.9580519618320033,
  "perf_predict_runtime_cpp_researcher_1": 0.49610859202221036,
  "timeit_runtime_cpp_researcher_1": 0.49437787929782645,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9816155988857939, \"recall\": 0.9475665501478893, \"f1-score\": 0.964290600629361, \"support\": 3719}, \"text\": {\"precision\": 0.8933260393873085, \"recall\": 0.9611536197763391, \"f1-score\": 0.9259994329458463, \"support\": 1699}, \"accuracy\": 0.9518272425249169, \"macro avg\": {\"precision\": 0.9374708191365512, \"recall\": 0.9543600849621141, \"f1-score\": 0.9451450167876037, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9539293748939285, \"recall\": 0.9518272425249169, \"f1-score\": 0.9522830897592445, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9451450167876037,
  "roc-auc_cpp_researcher_2": 0.9543600849621142,
  "perf_predict_runtime_cpp_researcher_2": 0.48810017900541425,
  "timeit_runtime_cpp_researcher_2": 0.5010669125011191,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9824361313868614, \"recall\": 0.9187286689419796, \"f1-score\": 0.9495149911816578, \"support\": 4688}, \"text\": {\"precision\": 0.8261068005476951, \"recall\": 0.9591944886062533, \"f1-score\": 0.887690044139284, \"support\": 1887}, \"accuracy\": 0.9303422053231939, \"macro avg\": {\"precision\": 0.9042714659672783, \"recall\": 0.9389615787741165, \"f1-score\": 0.9186025176604709, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9375702078441227, \"recall\": 0.9303422053231939, \"f1-score\": 0.9317714664563408, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9186025176604709,
  "roc-auc_java_researcher_1": 0.9389615787741165,
  "perf_predict_runtime_java_researcher_1": 0.44786866195499897,
  "timeit_runtime_java_researcher_1": 0.468923507805448,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9794801641586868, \"recall\": 0.9195205479452054, \"f1-score\": 0.9485537646279532, \"support\": 4672}, \"text\": {\"precision\": 0.8284671532846716, \"recall\": 0.9527806925498427, \"f1-score\": 0.8862859931673988, \"support\": 1906}, \"accuracy\": 0.9291577987230161, \"macro avg\": {\"precision\": 0.9039736587216791, \"recall\": 0.936150620247524, \"f1-score\": 0.917419878897676, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9357235818044951, \"recall\": 0.9291577987230161, \"f1-score\": 0.9305114459286804, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.917419878897676,
  "roc-auc_java_researcher_2": 0.9361506202475239,
  "perf_predict_runtime_java_researcher_2": 0.44792142196092755,
  "timeit_runtime_java_researcher_2": 0.4617238134029321,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9665692956832197, \"recall\": 0.9191358024691358, \"f1-score\": 0.9422559721563043, \"support\": 3240}, \"text\": {\"precision\": 0.874581139301101, \"recall\": 0.9466321243523316, \"f1-score\": 0.9091813884050759, \"support\": 1930}, \"accuracy\": 0.9294003868471954, \"macro avg\": {\"precision\": 0.9205752174921604, \"recall\": 0.9328839634107338, \"f1-score\": 0.92571868028069, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9322294229912489, \"recall\": 0.9294003868471954, \"f1-score\": 0.9299089805431765, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.92571868028069,
  "roc-auc_javascript_researcher_1": 0.9328839634107338,
  "perf_predict_runtime_javascript_researcher_1": 0.3075436700601131,
  "timeit_runtime_javascript_researcher_1": 0.3032444442040287,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9649008774780631, \"recall\": 0.9206201550387597, \"f1-score\": 0.9422405585528404, \"support\": 3225}, \"text\": {\"precision\": 0.877511961722488, \"recall\": 0.9443872296601442, \"f1-score\": 0.9097222222222223, \"support\": 1942}, \"accuracy\": 0.9295529320688988, \"macro avg\": {\"precision\": 0.9212064196002756, \"recall\": 0.932503692349452, \"f1-score\": 0.9259813903875314, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9320560401648588, \"recall\": 0.9295529320688988, \"f1-score\": 0.930018648517218, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9259813903875314,
  "roc-auc_javascript_researcher_2": 0.9325036923494519,
  "perf_predict_runtime_javascript_researcher_2": 0.30274399009067565,
  "timeit_runtime_javascript_researcher_2": 0.3271400009980425,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9764085484318623, \"recall\": 0.8324656885944155, \"f1-score\": 0.8987099246391621, \"support\": 4226}, \"text\": {\"precision\": 0.7263239273289525, \"recall\": 0.9567209775967414, \"f1-score\": 0.8257525818501429, \"support\": 1964}, \"accuracy\": 0.8718901453957997, \"macro avg\": {\"precision\": 0.8513662378804074, \"recall\": 0.8945933330955784, \"f1-score\": 0.8622312532446526, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.897060213077078, \"recall\": 0.8718901453957997, \"f1-score\": 0.8755615851823554, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8622312532446526,
  "roc-auc_php_researcher_1": 0.8945933330955784,
  "perf_predict_runtime_php_researcher_1": 0.35970182600431144,
  "timeit_runtime_php_researcher_1": 0.3625271019991487,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9745222929936306, \"recall\": 0.8382563125297761, \"f1-score\": 0.901267767960046, \"support\": 4198}, \"text\": {\"precision\": 0.7366175329712956, \"recall\": 0.9537920642893019, \"f1-score\": 0.8312541037426133, \"support\": 1991}, \"accuracy\": 0.8754241396025206, \"macro avg\": {\"precision\": 0.8555699129824631, \"recall\": 0.896024188409539, \"f1-score\": 0.8662609358513297, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.8979883816663614, \"recall\": 0.8754241396025206, \"f1-score\": 0.8787443868876744, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8662609358513297,
  "roc-auc_php_researcher_2": 0.8960241884095389,
  "perf_predict_runtime_php_researcher_2": 0.3806652199709788,
  "timeit_runtime_php_researcher_2": 0.3677621660986915,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9839034205231388, \"recall\": 0.8792855430352433, \"f1-score\": 0.9286573400012661, \"support\": 8342}, \"text\": {\"precision\": 0.707777132907719, \"recall\": 0.9531066822977726, \"f1-score\": 0.8123230641132388, \"support\": 2559}, \"accuracy\": 0.8966149894505091, \"macro avg\": {\"precision\": 0.8458402767154289, \"recall\": 0.9161961126665079, \"f1-score\": 0.8704902020572525, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9190830214764588, \"recall\": 0.8966149894505091, \"f1-score\": 0.9013479727874818, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8704902020572525,
  "roc-auc_python_researcher_1": 0.9161961126665079,
  "perf_predict_runtime_python_researcher_1": 0.6558339769253507,
  "timeit_runtime_python_researcher_1": 0.6715485483990051,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9798328247313255, \"recall\": 0.8763498279340216, \"f1-score\": 0.9252067151089952, \"support\": 8427}, \"text\": {\"precision\": 0.697620429483459, \"recall\": 0.9405320813771518, \"f1-score\": 0.8010663112295902, \"support\": 2556}, \"accuracy\": 0.8912865337339525, \"macro avg\": {\"precision\": 0.8387266271073923, \"recall\": 0.9084409546555867, \"f1-score\": 0.8631365131692927, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9141554249085497, \"recall\": 0.8912865337339525, \"f1-score\": 0.8963163506989289, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8631365131692927,
  "roc-auc_python_researcher_2": 0.9084409546555867,
  "perf_predict_runtime_python_researcher_2": 0.6576105230487883,
  "timeit_runtime_python_researcher_2": 0.6759441814036109
}