{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 97.62764537299518,
  "model_size": 34.71734765625,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9722061378112333, \"recall\": 0.9056094929881338, \"f1-score\": 0.9377268919296285, \"support\": 3708}, \"text\": {\"precision\": 0.8216106014271152, \"recall\": 0.9437939110070258, \"f1-score\": 0.8784741144414169, \"support\": 1708}, \"accuracy\": 0.9176514032496307, \"macro avg\": {\"precision\": 0.8969083696191742, \"recall\": 0.9247017019975798, \"f1-score\": 0.9081005031855227, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9247140447270249, \"recall\": 0.9176514032496307, \"f1-score\": 0.9190408239920611, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9081005031855227,
  "roc-auc_cpp_researcher_1": 0.9247017019975797,
  "perf_predict_runtime_cpp_researcher_1": 0.4652243349701166,
  "timeit_runtime_cpp_researcher_1": 0.4695225664996542,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9722543352601156, \"recall\": 0.9045442323205163, \"f1-score\": 0.9371778799275666, \"support\": 3719}, \"text\": {\"precision\": 0.8186925434116445, \"recall\": 0.9434961742201294, \"f1-score\": 0.8766748701121136, \"support\": 1699}, \"accuracy\": 0.9167589516426726, \"macro avg\": {\"precision\": 0.8954734393358801, \"recall\": 0.9240202032703229, \"f1-score\": 0.9069263750198402, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9240997608137236, \"recall\": 0.9167589516426726, \"f1-score\": 0.9182050830142305, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9069263750198402,
  "roc-auc_cpp_researcher_2": 0.9240202032703229,
  "perf_predict_runtime_cpp_researcher_2": 0.7828454140108079,
  "timeit_runtime_cpp_researcher_2": 0.5055996621027589,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9756959787892179, \"recall\": 0.9419795221843004, \"f1-score\": 0.9585413501193836, \"support\": 4688}, \"text\": {\"precision\": 0.867252318204002, \"recall\": 0.9417064122946476, \"f1-score\": 0.9029471544715446, \"support\": 1887}, \"accuracy\": 0.9419011406844107, \"macro avg\": {\"precision\": 0.9214741484966099, \"recall\": 0.9418429672394739, \"f1-score\": 0.9307442522954641, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.944573060534571, \"recall\": 0.9419011406844107, \"f1-score\": 0.9425860273532282, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9307442522954641,
  "roc-auc_java_researcher_1": 0.941842967239474,
  "perf_predict_runtime_java_researcher_1": 0.49139040999580175,
  "timeit_runtime_java_researcher_1": 0.5284829186042771,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9730624862000442, \"recall\": 0.943279109589041, \"f1-score\": 0.957939354417998, \"support\": 4672}, \"text\": {\"precision\": 0.8706686188384578, \"recall\": 0.9359916054564533, \"f1-score\": 0.902149178255373, \"support\": 1906}, \"accuracy\": 0.9411675281240499, \"macro avg\": {\"precision\": 0.921865552519251, \"recall\": 0.9396353575227472, \"f1-score\": 0.9300442663366855, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9433934817623453, \"recall\": 0.9411675281240499, \"f1-score\": 0.9417739430823394, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9300442663366855,
  "roc-auc_java_researcher_2": 0.9396353575227473,
  "perf_predict_runtime_java_researcher_2": 0.45557274599559605,
  "timeit_runtime_java_researcher_2": 0.4739094564924017,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9689846555664381, \"recall\": 0.9160493827160494, \"f1-score\": 0.9417737585276853, \"support\": 3240}, \"text\": {\"precision\": 0.8709065021357381, \"recall\": 0.9507772020725389, \"f1-score\": 0.9090909090909091, \"support\": 1930}, \"accuracy\": 0.9290135396518375, \"macro avg\": {\"precision\": 0.9199455788510881, \"recall\": 0.9334132923942942, \"f1-score\": 0.9254323338092971, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9323713410362154, \"recall\": 0.9290135396518375, \"f1-score\": 0.9295730042891982, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9254323338092971,
  "roc-auc_javascript_researcher_1": 0.9334132923942942,
  "perf_predict_runtime_javascript_researcher_1": 0.30841193604283035,
  "timeit_runtime_javascript_researcher_1": 0.3219000631943345,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9660130718954248, \"recall\": 0.9165891472868217, \"f1-score\": 0.9406523468575976, \"support\": 3225}, \"text\": {\"precision\": 0.8723303274798292, \"recall\": 0.9464469618949537, \"f1-score\": 0.907878488515683, \"support\": 1942}, \"accuracy\": 0.9278111089607122, \"macro avg\": {\"precision\": 0.919171699687627, \"recall\": 0.9315180545908877, \"f1-score\": 0.9242654176866403, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9308027197268383, \"recall\": 0.9278111089607122, \"f1-score\": 0.9283343997122524, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9242654176866403,
  "roc-auc_javascript_researcher_2": 0.9315180545908879,
  "perf_predict_runtime_javascript_researcher_2": 0.32368725107517093,
  "timeit_runtime_javascript_researcher_2": 0.3224321851041168,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9758175322890904, \"recall\": 0.8402744912446758, \"f1-score\": 0.9029879211697392, \"support\": 4226}, \"text\": {\"precision\": 0.7353978831830654, \"recall\": 0.955193482688391, \"f1-score\": 0.8310077519379845, \"support\": 1964}, \"accuracy\": 0.8767366720516963, \"macro avg\": {\"precision\": 0.855607707736078, \"recall\": 0.8977339869665334, \"f1-score\": 0.8669978365538619, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.8995357567084388, \"recall\": 0.8767366720516963, \"f1-score\": 0.8801496251485492, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8669978365538619,
  "roc-auc_php_researcher_1": 0.8977339869665334,
  "perf_predict_runtime_php_researcher_1": 0.3678405840182677,
  "timeit_runtime_php_researcher_1": 0.3880182033055462,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9648597112503405, \"recall\": 0.8437351119580753, \"f1-score\": 0.9002414538060745, \"support\": 4198}, \"text\": {\"precision\": 0.7394757744241461, \"recall\": 0.9352084379708689, \"f1-score\": 0.8259037480594367, \"support\": 1991}, \"accuracy\": 0.8731620617224107, \"macro avg\": {\"precision\": 0.8521677428372434, \"recall\": 0.889471774964472, \"f1-score\": 0.8630726009327556, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.8923537461152697, \"recall\": 0.8731620617224107, \"f1-score\": 0.8763270294820228, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8630726009327556,
  "roc-auc_php_researcher_2": 0.8894717749644722,
  "perf_predict_runtime_php_researcher_2": 0.3675096849910915,
  "timeit_runtime_php_researcher_2": 0.3665260834968649,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9878755220261349, \"recall\": 0.8790457923759291, \"f1-score\": 0.9302886140183951, \"support\": 8342}, \"text\": {\"precision\": 0.7098907418056354, \"recall\": 0.9648300117233294, \"f1-score\": 0.81795593837999, \"support\": 2559}, \"accuracy\": 0.8991835611411797, \"macro avg\": {\"precision\": 0.8488831319158852, \"recall\": 0.9219379020496292, \"f1-score\": 0.8741222761991926, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9226188435026731, \"recall\": 0.8991835611411797, \"f1-score\": 0.9039186188841251, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8741222761991926,
  "roc-auc_python_researcher_1": 0.9219379020496292,
  "perf_predict_runtime_python_researcher_1": 0.6970248229335994,
  "timeit_runtime_python_researcher_1": 0.6980391461984254,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9845456967759126, \"recall\": 0.8769431588940311, \"f1-score\": 0.9276344693403628, \"support\": 8427}, \"text\": {\"precision\": 0.7017543859649122, \"recall\": 0.9546165884194053, \"f1-score\": 0.8088844687551798, \"support\": 2556}, \"accuracy\": 0.8950195757079122, \"macro avg\": {\"precision\": 0.8431500413704125, \"recall\": 0.9157798736567182, \"f1-score\": 0.8682594690477713, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9187335698130684, \"recall\": 0.8950195757079122, \"f1-score\": 0.8999985773713445, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8682594690477713,
  "roc-auc_python_researcher_2": 0.9157798736567182,
  "perf_predict_runtime_python_researcher_2": 0.6710805339971557,
  "timeit_runtime_python_researcher_2": 0.7169966346933506
}