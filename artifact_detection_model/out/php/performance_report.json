{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 100.59904279990587,
  "model_size": 36.5915732421875,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9786311551563952, \"recall\": 0.8522114347357066, \"f1-score\": 0.9110566527317284, \"support\": 3708}, \"text\": {\"precision\": 0.7494284407864655, \"recall\": 0.9596018735362998, \"f1-score\": 0.841591784338896, \"support\": 1708}, \"accuracy\": 0.8860782865583456, \"macro avg\": {\"precision\": 0.8640297979714304, \"recall\": 0.9059066541360032, \"f1-score\": 0.8763242185353122, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9063493538004425, \"recall\": 0.8860782865583456, \"f1-score\": 0.8891500804985383, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.8763242185353122,
  "roc-auc_cpp_researcher_1": 0.9059066541360032,
  "perf_predict_runtime_cpp_researcher_1": 0.49535306892357767,
  "timeit_runtime_cpp_researcher_1": 0.48230713289231064,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9777434312210201, \"recall\": 0.850497445549879, \"f1-score\": 0.9096922634454991, \"support\": 3719}, \"text\": {\"precision\": 0.7453046266605589, \"recall\": 0.9576221306650972, \"f1-score\": 0.8382277176713034, \"support\": 1699}, \"accuracy\": 0.8840900701365818, \"macro avg\": {\"precision\": 0.8615240289407895, \"recall\": 0.9040597881074881, \"f1-score\": 0.8739599905584012, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.904854260134231, \"recall\": 0.8840900701365818, \"f1-score\": 0.8872821004203314, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.8739599905584012,
  "roc-auc_cpp_researcher_2": 0.9040597881074881,
  "perf_predict_runtime_cpp_researcher_2": 0.4786647619912401,
  "timeit_runtime_cpp_researcher_2": 0.47645932170562444,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.985230024213075, \"recall\": 0.8679607508532423, \"f1-score\": 0.922885007938308, \"support\": 4688}, \"text\": {\"precision\": 0.7468302658486707, \"recall\": 0.9676735559088501, \"f1-score\": 0.843028624192059, \"support\": 1887}, \"accuracy\": 0.8965779467680608, \"macro avg\": {\"precision\": 0.8660301450308729, \"recall\": 0.9178171533810462, \"f1-score\": 0.8829568160651835, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9168102000254507, \"recall\": 0.8965779467680608, \"f1-score\": 0.8999665294395747, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.8829568160651835,
  "roc-auc_java_researcher_1": 0.9178171533810462,
  "perf_predict_runtime_java_researcher_1": 0.4581713629886508,
  "timeit_runtime_java_researcher_1": 0.44959677449660373,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9847531461761858, \"recall\": 0.8709332191780822, \"f1-score\": 0.924352567014993, \"support\": 4672}, \"text\": {\"precision\": 0.7534750613246116, \"recall\": 0.9669464847848899, \"f1-score\": 0.8469669117647058, \"support\": 1906}, \"accuracy\": 0.8987534204925509, \"macro avg\": {\"precision\": 0.8691141037503987, \"recall\": 0.918939851981486, \"f1-score\": 0.8856597393898494, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9177394596868121, \"recall\": 0.8987534204925509, \"f1-score\": 0.9019297851805378, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.8856597393898494,
  "roc-auc_java_researcher_2": 0.9189398519814861,
  "perf_predict_runtime_java_researcher_2": 0.5001104739494622,
  "timeit_runtime_java_researcher_2": 0.46952109039993956,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9635726795096322, \"recall\": 0.8490740740740741, \"f1-score\": 0.902707136997539, \"support\": 3240}, \"text\": {\"precision\": 0.7887688984881209, \"recall\": 0.9461139896373058, \"f1-score\": 0.860306242638398, \"support\": 1930}, \"accuracy\": 0.8852998065764023, \"macro avg\": {\"precision\": 0.8761707889988766, \"recall\": 0.8975940318556899, \"f1-score\": 0.8815066898179685, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.8983171094184297, \"recall\": 0.8852998065764023, \"f1-score\": 0.886878563281264, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.8815066898179685,
  "roc-auc_javascript_researcher_1": 0.89759403185569,
  "perf_predict_runtime_javascript_researcher_1": 0.30624833900947124,
  "timeit_runtime_javascript_researcher_1": 0.3027204646030441,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9635215713784637, \"recall\": 0.8517829457364341, \"f1-score\": 0.9042132982225148, \"support\": 3225}, \"text\": {\"precision\": 0.7936096718480138, \"recall\": 0.9464469618949537, \"f1-score\": 0.8633161108501644, \"support\": 1942}, \"accuracy\": 0.8873621056706019, \"macro avg\": {\"precision\": 0.8785656216132387, \"recall\": 0.899114953815694, \"f1-score\": 0.8837647045363396, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.8996607413246349, \"recall\": 0.8873621056706019, \"f1-score\": 0.8888422245091212, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.8837647045363396,
  "roc-auc_javascript_researcher_2": 0.8991149538156938,
  "perf_predict_runtime_javascript_researcher_2": 0.3336613749852404,
  "timeit_runtime_javascript_researcher_2": 0.31583483499707654,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9891792029559251, \"recall\": 0.8868906767628963, \"f1-score\": 0.9352464129756706, \"support\": 4226}, \"text\": {\"precision\": 0.8009162848812995, \"recall\": 0.9791242362525459, \"f1-score\": 0.8810996563573883, \"support\": 1964}, \"accuracy\": 0.9161550888529887, \"macro avg\": {\"precision\": 0.8950477439186123, \"recall\": 0.9330074565077211, \"f1-score\": 0.9081730346665294, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9294460250724735, \"recall\": 0.9161550888529887, \"f1-score\": 0.9180664081294174, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.9081730346665294,
  "roc-auc_php_researcher_1": 0.9330074565077211,
  "perf_predict_runtime_php_researcher_1": 0.36517125798854977,
  "timeit_runtime_php_researcher_1": 0.3841477200970985,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9701453104359313, \"recall\": 0.8747022391615055, \"f1-score\": 0.9199549041713642, \"support\": 4198}, \"text\": {\"precision\": 0.781198003327787, \"recall\": 0.9432446007031643, \"f1-score\": 0.8546075085324232, \"support\": 1991}, \"accuracy\": 0.896752302472128, \"macro avg\": {\"precision\": 0.8756716568818592, \"recall\": 0.9089734199323349, \"f1-score\": 0.8872812063518937, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.90936100142764, \"recall\": 0.896752302472128, \"f1-score\": 0.8989326607205431, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8872812063518937,
  "roc-auc_php_researcher_2": 0.9089734199323348,
  "perf_predict_runtime_php_researcher_2": 0.36421158304437995,
  "timeit_runtime_php_researcher_2": 0.37831812660442665,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9756583629893238, \"recall\": 0.821625509470151, \"f1-score\": 0.8920413873885599, \"support\": 8342}, \"text\": {\"precision\": 0.6160990712074303, \"recall\": 0.9331770222743259, \"f1-score\": 0.7421911421911421, \"support\": 2559}, \"accuracy\": 0.8478121273277681, \"macro avg\": {\"precision\": 0.7958787170983771, \"recall\": 0.8774012658722384, \"f1-score\": 0.817116264789851, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.8912521408381575, \"recall\": 0.8478121273277681, \"f1-score\": 0.8568641763565268, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.817116264789851,
  "roc-auc_python_researcher_1": 0.8774012658722384,
  "perf_predict_runtime_python_researcher_1": 0.6602688829880208,
  "timeit_runtime_python_researcher_1": 0.6639214208931662,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9721323011963406, \"recall\": 0.819627388157114, \"f1-score\": 0.8893896471800155, \"support\": 8427}, \"text\": {\"precision\": 0.6080453842186694, \"recall\": 0.9225352112676056, \"f1-score\": 0.7329810382343799, \"support\": 2556}, \"accuracy\": 0.8435764363106619, \"macro avg\": {\"precision\": 0.790088842707505, \"recall\": 0.8710812997123598, \"f1-score\": 0.8111853427071977, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.8874007925197561, \"recall\": 0.8435764363106619, \"f1-score\": 0.8529897196133175, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8111853427071977,
  "roc-auc_python_researcher_2": 0.8710812997123598,
  "perf_predict_runtime_python_researcher_2": 0.6623167749494314,
  "timeit_runtime_python_researcher_2": 0.6582833213033155
}