{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 64.74782902002335,
  "model_size": 42.0488603515625,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9840440963156368, \"recall\": 0.9147788565264293, \"f1-score\": 0.9481481481481482, \"support\": 3708}, \"text\": {\"precision\": 0.8395124428643982, \"recall\": 0.9677985948477752, \"f1-score\": 0.8991025292357901, \"support\": 1708}, \"accuracy\": 0.9314992614475628, \"macro avg\": {\"precision\": 0.9117782695900175, \"recall\": 0.9412887256871023, \"f1-score\": 0.9236253386919692, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9384643208180896, \"recall\": 0.9314992614475628, \"f1-score\": 0.9326810290376778, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.9236253386919692,
  "roc-auc_cpp_researcher_1": 0.9412887256871022,
  "perf_predict_runtime_cpp_researcher_1": 0.4720231450628489,
  "timeit_runtime_cpp_researcher_1": 0.47255165240494534,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9812680115273775, \"recall\": 0.9155687012637805, \"f1-score\": 0.9472805675337321, \"support\": 3719}, \"text\": {\"precision\": 0.8388090349075975, \"recall\": 0.9617422012948793, \"f1-score\": 0.8960789690156293, \"support\": 1699}, \"accuracy\": 0.9300479881875231, \"macro avg\": {\"precision\": 0.9100385232174875, \"recall\": 0.9386554512793299, \"f1-score\": 0.9216797682746807, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9365951061606359, \"recall\": 0.9300479881875231, \"f1-score\": 0.9312245476219092, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.9216797682746807,
  "roc-auc_cpp_researcher_2": 0.9386554512793299,
  "perf_predict_runtime_cpp_researcher_2": 0.48320495896041393,
  "timeit_runtime_cpp_researcher_2": 0.47893718869891017,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9839670178653229, \"recall\": 0.9163822525597269, \"f1-score\": 0.9489728296885355, \"support\": 4688}, \"text\": {\"precision\": 0.822544137618832, \"recall\": 0.9629040805511394, \"f1-score\": 0.88720703125, \"support\": 1887}, \"accuracy\": 0.9297338403041825, \"macro avg\": {\"precision\": 0.9032555777420774, \"recall\": 0.9396431665554332, \"f1-score\": 0.9180899304692678, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.9376392650097901, \"recall\": 0.9297338403041825, \"f1-score\": 0.9312462803876204, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9180899304692678,
  "roc-auc_java_researcher_1": 0.9396431665554332,
  "perf_predict_runtime_java_researcher_1": 0.4888122020056471,
  "timeit_runtime_java_researcher_1": 0.4574875515070744,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9821428571428571, \"recall\": 0.918236301369863, \"f1-score\": 0.9491150442477876, \"support\": 4672}, \"text\": {\"precision\": 0.8271493212669683, \"recall\": 0.9590766002098636, \"f1-score\": 0.8882410106899903, \"support\": 1906}, \"accuracy\": 0.9300699300699301, \"macro avg\": {\"precision\": 0.9046460892049126, \"recall\": 0.9386564507898634, \"f1-score\": 0.9186780274688889, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9372329028437625, \"recall\": 0.9300699300699301, \"f1-score\": 0.931476566296866, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9186780274688889,
  "roc-auc_java_researcher_2": 0.9386564507898633,
  "perf_predict_runtime_java_researcher_2": 0.5022655670763925,
  "timeit_runtime_java_researcher_2": 0.5026324208010919,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9771790808240888, \"recall\": 0.9515432098765432, \"f1-score\": 0.9641907740422204, \"support\": 3240}, \"text\": {\"precision\": 0.9220843672456576, \"recall\": 0.9626943005181348, \"f1-score\": 0.9419518377693282, \"support\": 1930}, \"accuracy\": 0.9557059961315281, \"macro avg\": {\"precision\": 0.9496317240348732, \"recall\": 0.957118755197339, \"f1-score\": 0.9530713059057743, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9566118086371695, \"recall\": 0.9557059961315281, \"f1-score\": 0.9558888113716824, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9530713059057743,
  "roc-auc_javascript_researcher_1": 0.957118755197339,
  "perf_predict_runtime_javascript_researcher_1": 0.31396407505963,
  "timeit_runtime_javascript_researcher_1": 0.3237591641023755,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.9771501110758489, \"recall\": 0.9547286821705426, \"f1-score\": 0.9658092848180678, \"support\": 3225}, \"text\": {\"precision\": 0.9275793650793651, \"recall\": 0.9629248197734295, \"f1-score\": 0.944921677614957, \"support\": 1942}, \"accuracy\": 0.9578091736017031, \"macro avg\": {\"precision\": 0.952364738077607, \"recall\": 0.9588267509719861, \"f1-score\": 0.9553654812165124, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9585191088066073, \"recall\": 0.9578091736017031, \"f1-score\": 0.9579587461711855, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9553654812165124,
  "roc-auc_javascript_researcher_2": 0.958826750971986,
  "perf_predict_runtime_javascript_researcher_2": 0.33602766098920256,
  "timeit_runtime_javascript_researcher_2": 0.33163772489642723,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9798676227247656, \"recall\": 0.8407477520113582, \"f1-score\": 0.9049923586347427, \"support\": 4226}, \"text\": {\"precision\": 0.7375195007800313, \"recall\": 0.9628309572301426, \"f1-score\": 0.8352473498233216, \"support\": 1964}, \"accuracy\": 0.8794830371567044, \"macro avg\": {\"precision\": 0.8586935617523984, \"recall\": 0.9017893546207504, \"f1-score\": 0.8701198542290322, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9029739698169371, \"recall\": 0.8794830371567044, \"f1-score\": 0.8828632475999073, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8701198542290322,
  "roc-auc_php_researcher_1": 0.9017893546207503,
  "perf_predict_runtime_php_researcher_1": 0.37375629995949566,
  "timeit_runtime_php_researcher_1": 0.3960605302010663,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.9772040648173579, \"recall\": 0.8475464506908051, \"f1-score\": 0.9077688480673555, \"support\": 4198}, \"text\": {\"precision\": 0.7488226059654631, \"recall\": 0.958312405826218, \"f1-score\": 0.8407138136153339, \"support\": 1991}, \"accuracy\": 0.8831798351914687, \"macro avg\": {\"precision\": 0.8630133353914105, \"recall\": 0.9029294282585116, \"f1-score\": 0.8742413308413447, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.903733797476249, \"recall\": 0.8831798351914687, \"f1-score\": 0.8861972575690561, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8742413308413447,
  "roc-auc_php_researcher_2": 0.9029294282585115,
  "perf_predict_runtime_php_researcher_2": 0.3712876409990713,
  "timeit_runtime_php_researcher_2": 0.3901552936993539,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9846428081722199, \"recall\": 0.8608247422680413, \"f1-score\": 0.9185801087304126, \"support\": 8342}, \"text\": {\"precision\": 0.6782150776053215, \"recall\": 0.9562329034779211, \"f1-score\": 0.7935787254742988, \"support\": 2559}, \"accuracy\": 0.8832217227777268, \"macro avg\": {\"precision\": 0.8314289428887707, \"recall\": 0.9085288228729812, \"f1-score\": 0.8560794171023557, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9127091724946955, \"recall\": 0.8832217227777268, \"f1-score\": 0.8892361458139467, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8560794171023557,
  "roc-auc_python_researcher_1": 0.9085288228729812,
  "perf_predict_runtime_python_researcher_1": 0.7099669550079852,
  "timeit_runtime_python_researcher_1": 0.6892312439042143,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9825084745762712, \"recall\": 0.8598552272457577, \"f1-score\": 0.9170991013795723, \"support\": 8427}, \"text\": {\"precision\": 0.6726718403547672, \"recall\": 0.9495305164319249, \"f1-score\": 0.7874756651524983, \"support\": 2556}, \"accuracy\": 0.8807247564417736, \"macro avg\": {\"precision\": 0.8275901574655192, \"recall\": 0.9046928718388413, \"f1-score\": 0.8522873832660354, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9104022707093711, \"recall\": 0.8807247564417736, \"f1-score\": 0.8869327075894967, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8522873832660354,
  "roc-auc_python_researcher_2": 0.9046928718388413,
  "perf_predict_runtime_python_researcher_2": 0.6654787790030241,
  "timeit_runtime_python_researcher_2": 0.7112297573010438
}