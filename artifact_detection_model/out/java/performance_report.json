{
  "train_samples": 200000,
  "params": "{'charrep__repl_all_caps': False, 'vect__ngram_range': (1, 3), 'vect__stop_words': None, 'vect__lowercase': False}",
  "perf_train_runtime": 72.2935306020081,
  "model_size": 42.3469140625,
  "seed": 42,
  "train_frac": 200000,
  "man_validation_samples_cpp_researcher_1": 5416,
  "classification_report_cpp_researcher_1": "{\"artifact\": {\"precision\": 0.9770011947431302, \"recall\": 0.8821467098166127, \"f1-score\": 0.9271541950113379, \"support\": 3708}, \"text\": {\"precision\": 0.7886847195357833, \"recall\": 0.9549180327868853, \"f1-score\": 0.8638771186440677, \"support\": 1708}, \"accuracy\": 0.905096011816839, \"macro avg\": {\"precision\": 0.8828429571394567, \"recall\": 0.918532371301749, \"f1-score\": 0.8955156568277027, \"support\": 5416}, \"weighted avg\": {\"precision\": 0.9176133550728663, \"recall\": 0.905096011816839, \"f1-score\": 0.9071990165705518, \"support\": 5416}}",
  "macro_f1_cpp_researcher_1": 0.8955156568277027,
  "roc-auc_cpp_researcher_1": 0.9185323713017489,
  "perf_predict_runtime_cpp_researcher_1": 0.4743461200268939,
  "timeit_runtime_cpp_researcher_1": 0.47603570129722356,
  "man_validation_samples_cpp_researcher_2": 5418,
  "classification_report_cpp_researcher_2": "{\"artifact\": {\"precision\": 0.9747249479631281, \"recall\": 0.8814197364883033, \"f1-score\": 0.9257271957074271, \"support\": 3719}, \"text\": {\"precision\": 0.7854014598540145, \"recall\": 0.949970570924073, \"f1-score\": 0.8598827916888652, \"support\": 1699}, \"accuracy\": 0.9029162052417866, \"macro avg\": {\"precision\": 0.8800632039085714, \"recall\": 0.9156951537061881, \"f1-score\": 0.8928049936981461, \"support\": 5418}, \"weighted avg\": {\"precision\": 0.9153560652947295, \"recall\": 0.9029162052417866, \"f1-score\": 0.9050794211729979, \"support\": 5418}}",
  "macro_f1_cpp_researcher_2": 0.8928049936981461,
  "roc-auc_cpp_researcher_2": 0.915695153706188,
  "perf_predict_runtime_cpp_researcher_2": 0.47888517403043807,
  "timeit_runtime_cpp_researcher_2": 0.4808722462039441,
  "man_validation_samples_java_researcher_1": 6575,
  "classification_report_java_researcher_1": "{\"artifact\": {\"precision\": 0.9878487848784878, \"recall\": 0.9364334470989761, \"f1-score\": 0.961454226894437, \"support\": 4688}, \"text\": {\"precision\": 0.8601595495072736, \"recall\": 0.9713831478537361, \"f1-score\": 0.9123942259830763, \"support\": 1887}, \"accuracy\": 0.9464638783269962, \"macro avg\": {\"precision\": 0.9240041671928807, \"recall\": 0.9539082974763562, \"f1-score\": 0.9369242264387567, \"support\": 6575}, \"weighted avg\": {\"precision\": 0.95120245983735, \"recall\": 0.9464638783269962, \"f1-score\": 0.9473741931728039, \"support\": 6575}}",
  "macro_f1_java_researcher_1": 0.9369242264387567,
  "roc-auc_java_researcher_1": 0.9539082974763562,
  "perf_predict_runtime_java_researcher_1": 0.47088467504363507,
  "timeit_runtime_java_researcher_1": 0.4836301185074262,
  "man_validation_samples_java_researcher_2": 6578,
  "classification_report_java_researcher_2": "{\"artifact\": {\"precision\": 0.9858331459410838, \"recall\": 0.9383561643835616, \"f1-score\": 0.961508937383485, \"support\": 4672}, \"text\": {\"precision\": 0.8648521820741436, \"recall\": 0.9669464847848899, \"f1-score\": 0.9130542482041121, \"support\": 1906}, \"accuracy\": 0.9466403162055336, \"macro avg\": {\"precision\": 0.9253426640076137, \"recall\": 0.9526513245842257, \"f1-score\": 0.9372815927937985, \"support\": 6578}, \"weighted avg\": {\"precision\": 0.9507784610626422, \"recall\": 0.9466403162055336, \"f1-score\": 0.9474690107225113, \"support\": 6578}}",
  "macro_f1_java_researcher_2": 0.9372815927937985,
  "roc-auc_java_researcher_2": 0.9526513245842257,
  "perf_predict_runtime_java_researcher_2": 0.4632732729660347,
  "timeit_runtime_java_researcher_2": 0.4644239844987169,
  "man_validation_samples_javascript_researcher_1": 5170,
  "classification_report_javascript_researcher_1": "{\"artifact\": {\"precision\": 0.9651579290133507, \"recall\": 0.9148148148148149, \"f1-score\": 0.9393123118364759, \"support\": 3240}, \"text\": {\"precision\": 0.8685088137208195, \"recall\": 0.944559585492228, \"f1-score\": 0.9049391908662199, \"support\": 1930}, \"accuracy\": 0.9259187620889748, \"macro avg\": {\"precision\": 0.9168333713670851, \"recall\": 0.9296872001535215, \"f1-score\": 0.9221257513513479, \"support\": 5170}, \"weighted avg\": {\"precision\": 0.9290780852000847, \"recall\": 0.9259187620889748, \"f1-score\": 0.9264805664839432, \"support\": 5170}}",
  "macro_f1_javascript_researcher_1": 0.9221257513513479,
  "roc-auc_javascript_researcher_1": 0.9296872001535215,
  "perf_predict_runtime_javascript_researcher_1": 0.30869922903366387,
  "timeit_runtime_javascript_researcher_1": 0.3088366625015624,
  "man_validation_samples_javascript_researcher_2": 5167,
  "classification_report_javascript_researcher_2": "{\"artifact\": {\"precision\": 0.964460384740789, \"recall\": 0.9172093023255814, \"f1-score\": 0.9402415766052129, \"support\": 3225}, \"text\": {\"precision\": 0.8728571428571429, \"recall\": 0.9438722966014418, \"f1-score\": 0.9069767441860466, \"support\": 1942}, \"accuracy\": 0.9272305012579833, \"macro avg\": {\"precision\": 0.9186587637989659, \"recall\": 0.9305407994635115, \"f1-score\": 0.9236091603956298, \"support\": 5167}, \"weighted avg\": {\"precision\": 0.9300316067771658, \"recall\": 0.9272305012579833, \"f1-score\": 0.9277390984635405, \"support\": 5167}}",
  "macro_f1_javascript_researcher_2": 0.9236091603956298,
  "roc-auc_javascript_researcher_2": 0.9305407994635116,
  "perf_predict_runtime_javascript_researcher_2": 0.3092871990520507,
  "timeit_runtime_javascript_researcher_2": 0.3092780638020486,
  "man_validation_samples_php_researcher_1": 6190,
  "classification_report_php_researcher_1": "{\"artifact\": {\"precision\": 0.9748427672955975, \"recall\": 0.8435873166114529, \"f1-score\": 0.9044779906127108, \"support\": 4226}, \"text\": {\"precision\": 0.7390446111330439, \"recall\": 0.9531568228105907, \"f1-score\": 0.8325550366911274, \"support\": 1964}, \"accuracy\": 0.8783521809369952, \"macro avg\": {\"precision\": 0.8569436892143207, \"recall\": 0.8983720697110218, \"f1-score\": 0.8685165136519191, \"support\": 6190}, \"weighted avg\": {\"precision\": 0.9000273264711621, \"recall\": 0.8783521809369952, \"f1-score\": 0.8816578482052811, \"support\": 6190}}",
  "macro_f1_php_researcher_1": 0.8685165136519191,
  "roc-auc_php_researcher_1": 0.8983720697110218,
  "perf_predict_runtime_php_researcher_1": 0.3696078920038417,
  "timeit_runtime_php_researcher_1": 0.3713417521095835,
  "man_validation_samples_php_researcher_2": 6189,
  "classification_report_php_researcher_2": "{\"artifact\": {\"precision\": 0.97573083287369, \"recall\": 0.8427822772748929, \"f1-score\": 0.9043967280163598, \"support\": 4198}, \"text\": {\"precision\": 0.7424892703862661, \"recall\": 0.9558011049723757, \"f1-score\": 0.8357487922705314, \"support\": 1991}, \"accuracy\": 0.8791404104055582, \"macro avg\": {\"precision\": 0.8591100516299781, \"recall\": 0.8992916911236343, \"f1-score\": 0.8700727601434457, \"support\": 6189}, \"weighted avg\": {\"precision\": 0.9006970712138967, \"recall\": 0.8791404104055582, \"f1-score\": 0.8823127015064318, \"support\": 6189}}",
  "macro_f1_php_researcher_2": 0.8700727601434457,
  "roc-auc_php_researcher_2": 0.8992916911236342,
  "perf_predict_runtime_php_researcher_2": 0.37216721090953797,
  "timeit_runtime_php_researcher_2": 0.3697298876009881,
  "man_validation_samples_python_researcher_1": 10901,
  "classification_report_python_researcher_1": "{\"artifact\": {\"precision\": 0.9818628119459976, \"recall\": 0.8631023735315272, \"f1-score\": 0.9186602870813397, \"support\": 8342}, \"text\": {\"precision\": 0.679932735426009, \"recall\": 0.9480265728800312, \"f1-score\": 0.7919046841847559, \"support\": 2559}, \"accuracy\": 0.8830382533712503, \"macro avg\": {\"precision\": 0.8308977736860033, \"recall\": 0.9055644732057793, \"f1-score\": 0.8552824856330479, \"support\": 10901}, \"weighted avg\": {\"precision\": 0.9109849965332234, \"recall\": 0.8830382533712503, \"f1-score\": 0.888904522673271, \"support\": 10901}}",
  "macro_f1_python_researcher_1": 0.8552824856330479,
  "roc-auc_python_researcher_1": 0.9055644732057793,
  "perf_predict_runtime_python_researcher_1": 0.6668482519453391,
  "timeit_runtime_python_researcher_1": 0.6687836979981512,
  "man_validation_samples_python_researcher_2": 10983,
  "classification_report_python_researcher_2": "{\"artifact\": {\"precision\": 0.9781464993929583, \"recall\": 0.8604485582057672, \"f1-score\": 0.915530303030303, \"support\": 8427}, \"text\": {\"precision\": 0.6705882352941176, \"recall\": 0.9366197183098591, \"f1-score\": 0.781586679725759, \"support\": 2556}, \"accuracy\": 0.8781753619229719, \"macro avg\": {\"precision\": 0.8243673673435379, \"recall\": 0.8985341382578131, \"f1-score\": 0.8485584913780311, \"support\": 10983}, \"weighted avg\": {\"precision\": 0.9065705253388167, \"recall\": 0.8781753619229719, \"f1-score\": 0.8843585010484752, \"support\": 10983}}",
  "macro_f1_python_researcher_2": 0.8485584913780311,
  "roc-auc_python_researcher_2": 0.898534138257813,
  "perf_predict_runtime_python_researcher_2": 0.6693113730289042,
  "timeit_runtime_python_researcher_2": 0.6680035377037712
}